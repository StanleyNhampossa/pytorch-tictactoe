{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\python3\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import tqdm\n",
    "import torch\n",
    "import collections\n",
    "from collections import namedtuple, deque\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class Replay(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(27, 1024)\n",
    "        self.layer2 = nn.Linear(1024, 512)\n",
    "        self.layer3 = nn.Linear(512, 256)\n",
    "        self.layer4 = nn.Linear(256, 9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        return self.layer4(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self, config: dict):\n",
    "        \"\"\"\n",
    "        Inicia a classe do game\n",
    "        Arguments:\n",
    "            Array config: Configurações de recompensas\n",
    "        Returns:\n",
    "            Void\n",
    "        \"\"\"\n",
    "\n",
    "        # Configurações do game\n",
    "        self.config  =  config\n",
    "\n",
    "        # Quantidade de jogadas por partida\n",
    "        self.played  =  0     \n",
    "\n",
    "        # Matriz da partida\n",
    "        self.game    =  []\n",
    "\n",
    "        # Situação do game\n",
    "        self.done   =  True\n",
    "\n",
    "        # Total de ações\n",
    "        self.spaceAction  =  9\n",
    "\n",
    "        self.lastAction   =  -1\n",
    "\n",
    "        self.loss = 0\n",
    "        self.winn = 1\n",
    "\n",
    "        # Represeta os jogadores na partida\n",
    "        self.pX      =  1\n",
    "        self.pO      =  2\n",
    "\n",
    "    def newGame(self, player='X'):\n",
    "        \"\"\"\n",
    "        Inicia uma nova partida\n",
    "        Arguments:\n",
    "            String player: Usuário que inicia a partida\n",
    "        Returns:\n",
    "            Void\n",
    "        \"\"\"\n",
    "        self.lastAction = -1\n",
    "        self.done    =  False\n",
    "        self.player  =  player        \n",
    "        self.game    =  np.zeros(9).astype(int)\n",
    "        self.played  =  0\n",
    "        return self.getObservable()\n",
    "\n",
    "    def getObservable(self):\n",
    "        \"\"\"\n",
    "        Retorna a matriz do game com a jogada anterior\n",
    "        Returns:\n",
    "            Array\n",
    "        \"\"\"\n",
    "        state  = self.game\n",
    "        #state  = np.append(state, self.lastAction)\n",
    "        \n",
    "        # Retorna o estado\n",
    "        return state        \n",
    "\n",
    "    def getActionSpace(self):\n",
    "        return self.spaceAction\n",
    "\n",
    "    def getAction(self, randomAction: bool = True) -> int:\n",
    "        \"\"\"\n",
    "        Retorna uma ação\n",
    "        Arguments:\n",
    "            Bool randomAction: Se a ação será gerada de forma randômica\n",
    "                               True  - Ação randômica\n",
    "                               False - Ação baseada em movimentos válidos\n",
    "        Returns:\n",
    "            Void\n",
    "        \"\"\"\n",
    "\n",
    "        # Verifica se a opção\n",
    "        if randomAction:\n",
    "            # Gera a ação\n",
    "            return random.randrange( self.spaceAction )\n",
    "\n",
    "        # Retorna uma ação baseada em movimentos válidos\n",
    "        return random.choice( self.getOnlyValidAction() )\n",
    "\n",
    "    def getOnlyValidAction(self) -> list:\n",
    "        \"\"\"\n",
    "        Retorna somente os locais válidos para jogar\n",
    "        Returns:\n",
    "            Array\n",
    "        \"\"\"\n",
    "\n",
    "        # Lista com os itens válidos\n",
    "        valid = []\n",
    "\n",
    "        # Contador da lista\n",
    "        j = 0\n",
    "\n",
    "        # Loop na matriz do jogo\n",
    "        for i in self.game:\n",
    "            \n",
    "            # Verifica as posições validas\n",
    "            if i == 0:\n",
    "\n",
    "                # Coloca o item na lista\n",
    "                valid.append(j)\n",
    "\n",
    "            # Incrementa o contador\n",
    "            j += 1\n",
    "\n",
    "        # Retorna  a lista dos itens válidos\n",
    "        return valid\n",
    "    \n",
    "    def checkWinner( self, player: str ) -> bool:\n",
    "        \"\"\"\n",
    "        Verifica se a partida tem um vencedor\n",
    "        Arguments:\n",
    "            String player: Usuário que será verificado\n",
    "        Returns:\n",
    "            Boolean\n",
    "        \"\"\"\n",
    "\n",
    "        # Obtem o valor do player repsentado na matriz\n",
    "        p = self.pX if player == 'X' else self.pO\n",
    "\n",
    "        # Seta a partida como terminada\n",
    "        self.done  =  True\n",
    "\n",
    "        # Verifica se houve um vencedor\n",
    "        if self.game[0] == p and self.game[1] == p and self.game[2] == p:\n",
    "            return True\n",
    "        elif self.game[3] == p and self.game[4] == p and self.game[5] == p:\n",
    "            return True\n",
    "        elif self.game[6] == p and self.game[7] == p and self.game[8] == p:\n",
    "            return True\n",
    "        elif self.game[0] == p and self.game[3] == p and self.game[6] == p:\n",
    "            return True\n",
    "        elif self.game[1] == p and self.game[4] == p and self.game[7] == p:\n",
    "            return True\n",
    "        elif self.game[2] == p and self.game[5] == p and self.game[8] == p:\n",
    "            return True\n",
    "        elif self.game[0] == p and self.game[4] == p and self.game[8] == p:\n",
    "            return True\n",
    "        elif self.game[2] == p and self.game[4] == p and self.game[6] == p:\n",
    "            return True\n",
    "\n",
    "        # Não venceu, então o jogo continua  \n",
    "        self.done  = False\n",
    "\n",
    "        # Retorna o boolean, informando que o jogo nao terminou\n",
    "        return False\n",
    "    \n",
    "    def checkPossibilityWon(self, p):\n",
    "        \"\"\"\n",
    "        Verifica se a partida tem um vencedor\n",
    "        Arguments:\n",
    "            String player: Usuário que será verificado\n",
    "        Returns:\n",
    "            Boolean\n",
    "        \"\"\"\n",
    "\n",
    "        odd =  0\n",
    "\n",
    "        if (self.game[0] == 0 and self.game[1] == p and self.game[2] == p) or \\\n",
    "           (self.game[0] == p and self.game[1] == 0 and self.game[2] == p) or \\\n",
    "           (self.game[0] == p and self.game[1] == p and self.game[2] == 0): \n",
    "            odd += 1\n",
    "         \n",
    "        if (self.game[3] == 0 and self.game[4] == p and self.game[5] == p) or \\\n",
    "           (self.game[3] == p and self.game[4] == 0 and self.game[5] == p) or \\\n",
    "           (self.game[3] == p and self.game[4] == p and self.game[5] == 0): \n",
    "            odd += 1\n",
    "\n",
    "        if (self.game[6] == 0 and self.game[7] == p and self.game[8] == p) or \\\n",
    "           (self.game[6] == p and self.game[7] == 0 and self.game[8] == p) or \\\n",
    "           (self.game[6] == p and self.game[7] == p and self.game[8] == 0): \n",
    "            odd += 1\n",
    "\n",
    "        if (self.game[0] == 0 and self.game[3] == p and self.game[6] == p) or \\\n",
    "           (self.game[0] == p and self.game[3] == 0 and self.game[6] == p) or \\\n",
    "           (self.game[0] == p and self.game[3] == p and self.game[6] == 0): \n",
    "            odd += 1\n",
    "\n",
    "        if (self.game[1] == 0 and self.game[4] == p and self.game[7] == p) or \\\n",
    "           (self.game[1] == p and self.game[4] == 0 and self.game[7] == p) or \\\n",
    "           (self.game[1] == p and self.game[4] == p and self.game[7] == 0): \n",
    "            odd += 1\n",
    "\n",
    "        if (self.game[2] == 0 and self.game[5] == p and self.game[8] == p) or \\\n",
    "           (self.game[2] == p and self.game[5] == 0 and self.game[8] == p) or \\\n",
    "           (self.game[2] == p and self.game[5] == p and self.game[8] == 0): \n",
    "            odd += 1\n",
    "\n",
    "        if (self.game[0] == 0 and self.game[4] == p and self.game[8] == p) or \\\n",
    "           (self.game[0] == p and self.game[4] == 0 and self.game[8] == p) or \\\n",
    "           (self.game[0] == p and self.game[4] == p and self.game[8] == 0): \n",
    "            odd += 1\n",
    "\n",
    "        if (self.game[2] == 0 and self.game[4] == p and self.game[6] == p) or \\\n",
    "           (self.game[2] == p and self.game[4] == 0 and self.game[6] == p) or \\\n",
    "           (self.game[2] == p and self.game[4] == p and self.game[6] == 0): \n",
    "            odd += 1\n",
    "\n",
    "        # Retorna o boolean, informando que o jogo nao terminou\n",
    "        return odd\n",
    "    \n",
    "    def getReward(self, player):\n",
    "        pAtual       =  self.pX if player == 'X' else self.pO\n",
    "        pAdversario  =  self.pO if player == 'X' else self.pX\n",
    "\n",
    "        chancesDeVencer  =  self.checkPossibilityWon( pAtual )\n",
    "        chancesDePerder  =  self.checkPossibilityWon( pAdversario )\n",
    "\n",
    "        # print( \"Antes da Jogada Loss: {}\".format( self.loss ) )\n",
    "        # print( \"Antes da Jogada Winn: {}\".format( self.winn ) )\n",
    "        # print( \"Depois da Jogada Loss: {}\".format( chancesDePerder ) )\n",
    "        # print( \"Depois da Jogada Winn: {}\".format( chancesDeVencer ) )\n",
    "\n",
    "        return (0.7*(chancesDePerder*(-1)))+(0.3*chancesDeVencer)\n",
    "\n",
    "        '''\n",
    "        if chancesDePerder > 0:\n",
    "            # print( \"Antes Reward: {}\".format(   (0.5*(self.loss*-1))+(0.3*(self.winn))  ))\n",
    "            # print( \"Depois Reward: {}\".format(   (0.5*(chancesDePerder*-1))+(0.3*(chancesDeVencer))      ))\n",
    "            return (0.5*(self.loss*-1))+(0.3*(self.winn)) + (0.5*(chancesDePerder*-1))+(0.3*(chancesDeVencer))\n",
    "        elif self.loss == 1 and chancesDePerder == 0:\n",
    "            return ((0.5*(self.loss*-1)+1)+(0.3*self.winn)) + ((0.5*(chancesDePerder*-1))+((0.3*chancesDeVencer)+(-0.3)))\n",
    "        return (0.5*(chancesDePerder*-1))+(0.3*chancesDeVencer)\n",
    "        '''\n",
    "\n",
    "    def getGameStatus(self):\n",
    "        if self.played == self.spaceAction:\n",
    "            self.done = True\n",
    "        return self.done\n",
    "    \n",
    "    def checkPlayerWinner(self, player: str) -> list:\n",
    "        \"\"\"\n",
    "        Verifica se a partida tem um vencedor\n",
    "        Returns:\n",
    "            list\n",
    "        \"\"\"\n",
    "\n",
    "        if self.checkWinner( player ):\n",
    "            # Player venceu\n",
    "            return [self.getObservable(), self.config['rewardPositive'], self.done]            \n",
    "        elif self.getGameStatus():\n",
    "            # Empate\n",
    "            return [self.getObservable(), self.config['rewardDraw'], self.done]\n",
    "\n",
    "        # Ninguém venceu e o jogo não terminou\n",
    "        #return [self.getObservable(), self.config['rewardEachStep'], self.done]\n",
    "        return [self.getObservable(), self.getReward( player ), self.done]\n",
    "\n",
    "    def step(self, action: int, player: str) -> list:\n",
    "        \"\"\"\n",
    "        Faz as jogadas na partida\n",
    "        Arguments:\n",
    "            int action: Ação executada no ambiente\n",
    "            str player: Player que executa a ação\n",
    "        Returns:\n",
    "            list\n",
    "        \"\"\"\n",
    "\n",
    "        # Determina o jogador\n",
    "        p   =  self.pX if player == 'X' else self.pO\n",
    "\n",
    "        self.lastAction  =  action\n",
    "\n",
    "        self.played += 1\n",
    "\n",
    "        # Verifica se é uma ação valida\n",
    "        if self.game[action] == 0:\n",
    "\n",
    "            pAtual       =  self.pX if player == 'X' else self.pO\n",
    "            pAdversario  =  self.pO if player == 'X' else self.pX\n",
    "\n",
    "            self.winn  =  self.checkPossibilityWon( pAtual )\n",
    "            self.loss  =  self.checkPossibilityWon( pAdversario )\n",
    "            \n",
    "            # Faz a jogada na posição da matriz\n",
    "            self.game[action] = p   \n",
    "\n",
    "            # Retorna o status\n",
    "            return self.checkPlayerWinner( player )\n",
    "\n",
    "        # Finaliza o jogo\n",
    "        self.done = True\n",
    "\n",
    "        # Movimento inválido\n",
    "        return [self.getObservable(), self.config['rewardInvalidStep'], self.done]\n",
    "\n",
    "    def render(self):\n",
    "        currentGame = ''\n",
    "        separator   = ''\n",
    "        for arr in self.game.reshape(3, 3):\n",
    "            currentGame += '\\n'\n",
    "            j = 0\n",
    "            for i in arr:\n",
    "                separator = ' | ' if j < 2 else ''\n",
    "                if i == self.pX:\n",
    "                    currentGame += 'X'+separator\n",
    "                elif i == self.pO:\n",
    "                    currentGame += 'O'+separator\n",
    "                else:\n",
    "                    currentGame += ' '+separator\n",
    "                j += 1\n",
    "        return currentGame\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__( self, spaceAction ):\n",
    "        self.spaceAction = spaceAction\n",
    "\n",
    "    def chooseAction(self):\n",
    "        return random.randrange( self.spaceAction )\n",
    "    \n",
    "    def selectAction(self, actions, board):\n",
    "        p = 2\n",
    "        ad = 1\n",
    "        if (board[0] == 0 and board[1] == p and board[2] == p):\n",
    "            return 0\n",
    "        elif (board[0] == p and board[1] == 0 and board[2] == p):\n",
    "            return 1\n",
    "        elif (board[0] == p and board[1] == p and board[2] == 0):\n",
    "            return 2\n",
    "        elif(board[3] == 0 and board[4] == p and board[5] == p):\n",
    "            return 3\n",
    "        elif (board[3] == p and board[4] == 0 and board[5] == p):\n",
    "            return 4\n",
    "        elif (board[3] == p and board[4] == p and board[5] == 0):\n",
    "            return 5\n",
    "        elif(board[6] == 0 and board[7] == p and board[8] == p):\n",
    "            return 6\n",
    "        elif (board[6] == p and board[7] == 0 and board[8] == p):\n",
    "            return 7\n",
    "        elif (board[6] == p and board[7] == p and board[8] == 0):\n",
    "            return 8\n",
    "        elif(board[0] == 0 and board[3] == p and board[6] == p):\n",
    "            return 0\n",
    "        elif (board[0] == p and board[3] == 0 and board[6] == p):\n",
    "            return 3\n",
    "        elif (board[0] == p and board[3] == p and board[6] == 0):\n",
    "            return 6\n",
    "        elif(board[1] == 0 and board[4] == p and board[7] == p):\n",
    "            return 1\n",
    "        elif (board[1] == p and board[4] == 0 and board[7] == p):\n",
    "            return 4\n",
    "        elif (board[1] == p and board[4] == p and board[7] == 0):\n",
    "            return 7\n",
    "        elif(board[2] == 0 and board[5] == p and board[8] == p):\n",
    "            return 2\n",
    "        elif (board[2] == p and board[5] == 0 and board[8] == p):\n",
    "            return 5\n",
    "        elif (board[2] == p and board[5] == p and board[8] == 0):\n",
    "            return 8\n",
    "        elif(board[0] == 0 and board[4] == p and board[8] == p):\n",
    "            return 0\n",
    "        elif (board[0] == p and board[4] == 0 and board[8] == p):\n",
    "            return 4\n",
    "        elif (board[0] == p and board[4] == p and board[8] == 0):\n",
    "            return 8\n",
    "        elif(board[2] == 0 and board[4] == p and board[6] == p):\n",
    "            return 2\n",
    "        elif (board[2] == p and board[4] == 0 and board[6] == p):\n",
    "            return 4\n",
    "        elif (board[2] == p and board[4] == p and board[6] == 0):\n",
    "            return 6\n",
    "        else:\n",
    "            return random.choice( actions )\n",
    "\n",
    "\n",
    "        '''\n",
    "        elif (board[0] == 0 and board[1] == ad and board[2] == ad):\n",
    "            return 0        \n",
    "        elif (board[0] == ad and board[1] == 0 and board[2] == ad):\n",
    "            return 1\n",
    "        elif (board[0] == ad and board[1] == ad and board[2] == 0):\n",
    "            return 2\n",
    "        elif(board[3] == 0 and board[4] == ad and board[5] == ad):\n",
    "            return 3\n",
    "        elif (board[3] == ad and board[4] == 0 and board[5] == ad):\n",
    "            return 4\n",
    "        elif (board[3] == ad and board[4] == ad and board[5] == 0):\n",
    "            return 5\n",
    "        elif(board[6] == 0 and board[7] == ad and board[8] == ad):\n",
    "            return 6\n",
    "        elif (board[6] == ad and board[7] == 0 and board[8] == ad):\n",
    "            return 7\n",
    "        elif (board[6] == ad and board[7] == ad and board[8] == 0):\n",
    "            return 8\n",
    "        elif(board[0] == 0 and board[3] == ad and board[6] == ad):\n",
    "            return 0\n",
    "        elif (board[0] == ad and board[3] == 0 and board[6] == ad):\n",
    "            return 3\n",
    "        elif (board[0] == ad and board[3] == ad and board[6] == 0):\n",
    "            return 6\n",
    "        elif(board[1] == 0 and board[4] == ad and board[7] == ad):\n",
    "            return 1\n",
    "        elif (board[1] == ad and board[4] == 0 and board[7] == ad):\n",
    "            return 4\n",
    "        elif (board[1] == ad and board[4] == ad and board[7] == 0):\n",
    "            return 7\n",
    "        elif(board[2] == 0 and board[5] == ad and board[8] == ad):\n",
    "            return 2\n",
    "        elif (board[2] == ad and board[5] == 0 and board[8] == ad):\n",
    "            return 5\n",
    "        elif (board[2] == ad and board[5] == ad and board[8] == 0):\n",
    "            return 8\n",
    "        elif(board[0] == 0 and board[4] == ad and board[8] == ad):\n",
    "            return 0\n",
    "        elif (board[0] == ad and board[4] == 0 and board[8] == ad):\n",
    "            return 4\n",
    "        elif (board[0] == ad and board[4] == ad and board[8] == 0):\n",
    "            return 8\n",
    "        elif(board[2] == 0 and board[4] == ad and board[6] == ad):\n",
    "            return 2\n",
    "        elif (board[2] == ad and board[4] == 0 and board[6] == ad):\n",
    "            return 4\n",
    "        elif (board[2] == ad and board[4] == ad and board[6] == 0):\n",
    "            return 6            \n",
    "        else:\n",
    "            return random.choice( actions )\n",
    "        '''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNAgent:\n",
    "    def __init__(self,\n",
    "                 device,\n",
    "                 replaySize,\n",
    "                 inputSize,\n",
    "                 hiddenSize,\n",
    "                 outputSize,\n",
    "                 batchSize,\n",
    "                 gamma,\n",
    "                 epsStart,\n",
    "                 epsEnd,\n",
    "                 epsDecay,\n",
    "                 tau,\n",
    "                 lr,\n",
    "                 mode='train'):\n",
    "\n",
    "        self.mode = mode\n",
    "        self.device = device\n",
    "\n",
    "        # Define os parâmetros de epsilon (Estratégia gananciosa de epsilon)\n",
    "        self.epsStart = epsStart\n",
    "        self.epsEnd   = epsEnd\n",
    "        self.epsDecay = epsDecay\n",
    "\n",
    "        # Tamanho dos batch\n",
    "        self.batchSize  = batchSize\n",
    "\n",
    "        # Seta o gama\n",
    "        self.gamma  =  gamma\n",
    "\n",
    "        # Define o modelo\n",
    "        self.policy_net = CNeuralNetwork().to( self.device )\n",
    "        self.target_net = CNeuralNetwork().to( self.device )\n",
    "        \n",
    "        # Iguala os pesos\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "        # Configura o otimizador\n",
    "        self.optimizer  =  optim.AdamW( self.policy_net.parameters(), lr=lr, amsgrad=True)\n",
    "\n",
    "        # Loss Functions\n",
    "        self.criterion  =  nn.SmoothL1Loss()\n",
    "\n",
    "        # Configura o replay de experiência\n",
    "        self.memory  =  Replay( replaySize )\n",
    "\n",
    "        self.stepDone = 0\n",
    "\n",
    "        self.envActionSpace = None\n",
    "\n",
    "        self.eps_threshold = 0\n",
    "    \n",
    "    def saveModel(self, path, policyModel, targetModel):\n",
    "\n",
    "        # Salva o modelo\n",
    "        torch.save( self.policy_net.state_dict(), path+'/'+policyModel )\n",
    "        torch.save( self.target_net.state_dict(), path+'/'+targetModel )\n",
    "\n",
    "    def loadModel(self, path, policyModel, targetModel):\n",
    "\n",
    "        # Faz o carregamento de um modelo pré treinado\n",
    "        self.policy_net.load_state_dict( torch.load( path+'/'+policyModel )) \n",
    "        self.target_net.load_state_dict( torch.load( path+'/'+targetModel ))\n",
    "    \n",
    "    def setEnvActionSpace(self, envActionSpace):\n",
    "        self.envActionSpace  =  envActionSpace\n",
    "\n",
    "    def selectAction(self, state, actions, randomAction=True):\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            sample = random.random()\n",
    "            eps_threshold = self.epsEnd + (self.epsStart - self.epsEnd) * \\\n",
    "                math.exp(-1. * self.stepDone / self.epsDecay)\n",
    "\n",
    "            # Incremente as jogadas\n",
    "            self.stepDone += 1\n",
    "\n",
    "            self.eps_threshold = eps_threshold\n",
    "\n",
    "            # Verifica a proposta do epsilon e-greedy\n",
    "            if sample > eps_threshold or not randomAction:\n",
    "\n",
    "                # Sem computar gradientes, retorna a ação\n",
    "                with torch.no_grad():\n",
    "                    # t.max(1) will return largest column value of each row.\n",
    "                    # second column on max result is index of where max element was\n",
    "                    # found, so we pick action with the larger expected reward.\n",
    "                    return self.policy_net(state).max(1)[1].view(1, 1)\n",
    "            else:\n",
    "                return torch.tensor([[random.choice( actions )]], device=self.device, dtype=torch.long)\n",
    "                #return torch.tensor([[random.randrange( self.envActionSpace )]], device=self.device, dtype=torch.long)\n",
    "        else:\n",
    "            # Sem computar gradientes, retorna a ação\n",
    "            with torch.no_grad():\n",
    "                # t.max(1) will return largest column value of each row.\n",
    "                # second column on max result is index of where max element was\n",
    "                # found, so we pick action with the larger expected reward.\n",
    "                return self.policy_net(state).max(1)[1].view(1, 1)            \n",
    "    \n",
    "    def getThreshold(self):\n",
    "        return self.eps_threshold\n",
    "\n",
    "    def store(self, state, nextState, action, reward, done):\n",
    "        # Transforma para tensor\n",
    "        reward = torch.tensor([reward], device=self.device)\n",
    "\n",
    "        '''\n",
    "        # Verifica se a partida terminou\n",
    "        if done:\n",
    "            # Se sim, seta como None o próximo estado\n",
    "            next_state = None\n",
    "        else:\n",
    "            # Se não, transforma o \n",
    "            #next_state = torch.tensor(nextState, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "            next_state = self.prepareStateTensor( nextState )\n",
    "        '''\n",
    "        next_state = self.prepareStateTensor( nextState )\n",
    "\n",
    "        # Store the transition in memory\n",
    "        self.memory.push(state, action, next_state, reward)\n",
    "\n",
    "    def loadWeights(self):\n",
    "        # Iguala os pesos\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    def prepareStateTensor( self, state ):\n",
    "        tState = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        tState = F.one_hot(tState.long(), num_classes=3).unsqueeze(0)\n",
    "        return torch.reshape(tState, (-1,)).unsqueeze(0).float()\n",
    "\n",
    "    def optimize(self):\n",
    "        if len( self.memory ) <  self.batchSize :\n",
    "            return 0\n",
    "        \n",
    "        for _ in range(10):\n",
    "            transitions = self.memory.sample( self.batchSize )\n",
    "            # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "            # detailed explanation). This converts batch-array of Transitions\n",
    "            # to Transition of batch-arrays.\n",
    "            batch = Transition(*zip(*transitions))\n",
    "\n",
    "            # Compute a mask of non-final states and concatenate the batch elements\n",
    "            # (a final state would've been the one after which simulation ended)\n",
    "            non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                                batch.next_state)), device=self.device, dtype=torch.bool)\n",
    "            non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                        if s is not None])\n",
    "\n",
    "            state_batch = torch.cat(batch.state)\n",
    "            action_batch = torch.cat(batch.action)\n",
    "            reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "            # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "            # columns of actions taken. These are the actions which would've been taken\n",
    "            # for each batch state according to policy_net\n",
    "            d  =  self.policy_net(state_batch)\n",
    "\n",
    "            state_action_values  = d.gather(1, action_batch)\n",
    "            #state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "            # Compute V(s_{t+1}) for all next states.\n",
    "            # Expected values of actions for non_final_next_states are computed based\n",
    "            # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "            # This is merged based on the mask, such that we'll have either the expected\n",
    "            # state value or 0 in case the state was final.\n",
    "            next_state_values = torch.zeros(self.batchSize, device=self.device)\n",
    "            \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()\n",
    "\n",
    "            # Compute the expected Q values\n",
    "            expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
    "\n",
    "            # Optimize the model\n",
    "            self.policy_net.train()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Calcula o erro\n",
    "            loss = self.criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "            # Calcula a derivada do gradiente\n",
    "            loss.backward()\n",
    "\n",
    "            # In-place gradient clipping\n",
    "            torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Loss\n",
    "        return loss.item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica se existe GPU ativa\n",
    "isGpu    =  torch.cuda.is_available()\n",
    "\n",
    "# Determina o dispositivo (GPU ou CPU)\n",
    "device   =  torch.device(\"cuda\")\n",
    "\n",
    "# Obtem o path atual\n",
    "pwd      =  os.path.join(os.getcwd(), \"../\")\n",
    "\n",
    "# TrainID\n",
    "trainId  = 'JupyterNotebook20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config do treinamento\n",
    "config   =  {\n",
    "    \"device\": 'GPU',\n",
    "    \"maxEpisodes\": 100000,\n",
    "    \"trainEpisodesModule\": 5000,\n",
    "    \"replaySize\": 300000,\n",
    "    \"batchSize\": 10000,\n",
    "    \"inputSize\": 27,\n",
    "    \"hiddenSize\": 64,\n",
    "    \"outputSize\": 9,    \n",
    "    \"gamma\": 0.999,\n",
    "    \"epsStart\": 0.9,\n",
    "    \"epsEnd\": 0.05,\n",
    "    \"epsDecay\": 1000,\n",
    "    \"tau\": 0.005,\n",
    "    \"lr\": 0.001,\n",
    "    \"stateImage\": False,\n",
    "    \"rewardPositive\": 1,\n",
    "    \"rewardNegative\": -1,\n",
    "    \"rewardEachStep\": 0,\n",
    "    \"rewardErrorStep\": -1,\n",
    "    \"rewardInvalidStep\": -1,\n",
    "    \"rewardDraw\": 0.4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração do ambiente\n",
    "configGame = {\n",
    "    'stateImage': config[\"stateImage\"],\n",
    "    'stateImagePath': pwd+'/logs/games/',\n",
    "    'font': pwd+'/fonts/roboto-mono-bold.ttf',\n",
    "    'pathBoardFile': pwd+'/logs/game-{}.txt'.format(trainId),\n",
    "    \"rewardPositive\": config[\"rewardPositive\"],\n",
    "    \"rewardNegative\": config[\"rewardNegative\"],\n",
    "    \"rewardEachStep\": config[\"rewardEachStep\"],\n",
    "    \"rewardErrorStep\": config[\"rewardErrorStep\"],\n",
    "    \"rewardInvalidStep\": config[\"rewardInvalidStep\"],\n",
    "    \"rewardDraw\": config[\"rewardDraw\"]\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instancing Agents/Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancia o ambiente\n",
    "env     =   TicTacToe( configGame )\n",
    "\n",
    "# Define o agente randomico \n",
    "agent0  =   RandomAgent( spaceAction=env.getActionSpace() )\n",
    "\n",
    "# Define o agente com redes neurais\n",
    "agent1  =   CNNAgent(device, replaySize=config[\"replaySize\"], inputSize=config[\"inputSize\"], hiddenSize=config[\"hiddenSize\"], outputSize=config[\"outputSize\"],\n",
    "                             batchSize=config[\"batchSize\"], gamma=config[\"gamma\"], epsStart=config[\"epsStart\"], epsEnd=config[\"epsEnd\"], epsDecay=config[\"epsDecay\"],\n",
    "                             tau=config[\"tau\"], lr=config[\"lr\"], mode='train')\n",
    "\n",
    "# Seta o limite de ações do game no agente\n",
    "agent1.setEnvActionSpace( env.getActionSpace() )\n",
    "agent1.loadModel( pwd+'/models', '{}_policy_model.pth'.format(trainId), '{}_target_model.pth'.format(trainId) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliaJogadas(jogadas):\n",
    "    for i in reversed(range(len(jogadas))):\n",
    "        if i < len(jogadas)-1:\n",
    "            jogadas[i] += (jogadas[i+1]*(-1))\n",
    "    return jogadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showScores(scores, name, color='C1', figure=0, save=True):\n",
    "    plt.figure(figure)\n",
    "    plt.clf()\n",
    "    plt.title('Learning TicTacToe')\n",
    "    plt.xlabel('Sessions')\n",
    "    plt.ylabel('Mean Rewards')\n",
    "    plt.plot(scores, color=color)\n",
    "    if save:\n",
    "        plt.savefig(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodesReward: collections.deque = collections.deque(\n",
    "    maxlen=1000)\n",
    "\n",
    "episodesLoss: collections.deque = collections.deque(\n",
    "    maxlen=1000)\n",
    "\n",
    "games = ''\n",
    "scoresReward = []\n",
    "numEpisodes = 500000\n",
    "maxSteps = 9"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 499999: 100%|██████████| 500000/500000 [1:52:31<00:00, 74.06it/s, episode_reward=1.300, running_reward=1.791]   \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lastAction = -1\n",
    "\n",
    "# Loop de treinamento do agente\n",
    "with tqdm.trange( numEpisodes ) as episodes:  \n",
    "\n",
    "    for episode in episodes:\n",
    "\n",
    "        episodeReward = 0\n",
    "        \n",
    "        # Inicializa uma nova partida\n",
    "        s      =  env.newGame()\n",
    "\n",
    "        # Transforma o estado em tensor\n",
    "        state  =  agent1.prepareStateTensor( s )\n",
    "\n",
    "        # Determina quem começa a jogar\n",
    "        player =  'X' if episode % 2 == 0 else 'O'\n",
    "\n",
    "        jogadas = []\n",
    "\n",
    "        tdJogadas = []\n",
    "\n",
    "        playerO = 0\n",
    "\n",
    "        # Numero de jogadas por partida\n",
    "        for t in range(maxSteps):\n",
    "\n",
    "            # Verifica se o jogador é o \"X\"\n",
    "            if player == 'X':\n",
    "                # A rede neural retorna a melhor ação para essa jogada\n",
    "                sAction  =  agent1.selectAction( state, env.getOnlyValidAction() )\n",
    "\n",
    "                # Obtem o valor da ação\n",
    "                action  =  sAction.item()                \n",
    "            else:\n",
    "                # Agente pseudo aleatório\n",
    "                # Seleciona uma ação randomica com opções válidas disponíveis no tabuleiro\n",
    "                if t == 0 :\n",
    "                    if lastAction == 8:\n",
    "                        lastAction = 0\n",
    "                    else:\n",
    "                        lastAction += 1\n",
    "                    action  =  lastAction\n",
    "                else:\n",
    "                    action  =  agent0.selectAction( env.getOnlyValidAction(), s )\n",
    "\n",
    "            # Executa a ação no ambiente\n",
    "            nextState, reward, done = env.step( action, player )\n",
    "            tdJogadas.append( reward )\n",
    "            s  =  nextState\n",
    "\n",
    "            # Verifica se é o jogador \"O\", vamos guardar as ações dele\n",
    "            if player == 'X':\n",
    "                \n",
    "                # Adiciona a recompensa\n",
    "                episodeReward  +=  reward\n",
    "\n",
    "                # Armazena a experiência do agente para treinamento\n",
    "                jogadas.append([state, nextState, sAction, reward, done])\n",
    "                #agent1.store(state, nextState, sAction, reward, done)           \n",
    "            \n",
    "            # Seta o estado\n",
    "            state   =  agent1.prepareStateTensor( nextState )\n",
    "\n",
    "            # Verifica se terminou\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            # Troca o player para a próxima rodada do game\n",
    "            player =  'X' if player == 'O' else 'O'\n",
    "        \n",
    "        games += env.render()+\"\\n\"\n",
    "\n",
    "        # Calcula o TD Lambda das recompensas\n",
    "        tdJogadas  =  avaliaJogadas( tdJogadas )\n",
    "        index      =  0 if episode % 2 == 0 else 1\n",
    "        j          =  0\n",
    "        for i in range(len(tdJogadas)):\n",
    "            if i % 2 == index:\n",
    "                _reward  =  tdJogadas[i]\n",
    "                _state, _nextState, _sAction, r, _done  = jogadas[j]\n",
    "                agent1.store(_state, _nextState, _sAction, _reward, _done)\n",
    "                j += 1\n",
    "        \n",
    "\n",
    "        #for k in range(len(jogadas)):\n",
    "        #    _state, _nextState, _sAction, _reward, _done  = jogadas[k]\n",
    "        #    agent1.store(_state, _nextState, _sAction, _reward, _done)\n",
    "\n",
    "        if episode > 0 and episode % config[\"trainEpisodesModule\"] == 0:\n",
    "            # Otimiza a rede neural\n",
    "            agent1.optimize()\n",
    "\n",
    "        if episode > 0 and episode % 50000 == 0:\n",
    "            # Otimiza a rede neural\n",
    "            agent1.saveModel( pwd+'/models', '{}_policy_model.pth'.format(trainId), '{}_target_model.pth'.format(trainId) )\n",
    "            showScores( scoresReward, pwd+'/logs/{}.jpg'.format(trainId) )\n",
    "\n",
    "            f = open( pwd+'/logs/game-{}.txt'.format(trainId), 'a' )\n",
    "            f.write(games)\n",
    "            f.close()\n",
    "            games = ''\n",
    "\n",
    "        # Insere a recompensa do episódio\n",
    "        episodesReward.append( episodeReward )\n",
    "\n",
    "        # Dados para gráfico de scores\n",
    "        scoresReward.append( sum( episodesReward ) / (len( episodesReward ) + 1.) )\n",
    "\n",
    "        # Calcula a média de recompensas\n",
    "        runningReward  =  statistics.mean( episodesReward )    \n",
    "\n",
    "        # Atualiza a contagem\n",
    "        episodes.set_description(f'Episode {episode}')\n",
    "\n",
    "        # Exibe os dados\n",
    "        episodes.set_postfix(\n",
    "            episode_reward=\"{:.3f}\".format( episodeReward ),\n",
    "            running_reward=\"{:.3f}\".format( runningReward ))\n",
    "\n",
    "# Save model\n",
    "agent1.saveModel( pwd+'/models', '{}_policy_model.pth'.format(trainId), '{}_target_model.pth'.format(trainId) )\n",
    "showScores( scoresReward, pwd+'/logs/{}.jpg'.format(trainId) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
